name: Manual TestRun
run-name: 🚀🧐🚀 Only Tests (on demand) 🚀🧐🚀

# Workflow runs when manually triggered using the UI or API
on:
  workflow_dispatch:
    # Inputs the workflow accepts
    inputs:
      os:
        # Prompt to be shown in the UI
        description: Which OS to run tests on?
        # Default value if no value is explicitly provided
        default: ubuntu-latest
        # Input has to be provided for the workflow to run
        required: false
        # The data type of the input
        type: choice
        options:
          - ubuntu-latest
          - windows-latest
          - macos-latest
      message:
        description: Push to Start...
        default: ⬇️ Push button below to Run Test! ⬇️
        required: false
        type: string

concurrency:
  group: ${{ github.actor }} || Manual TestRun
  cancel-in-progress: true

permissions:
  contents: read

defaults:
  run:
    shell: bash

jobs:
  test:
    name: Test Run
    env:
      OS_TEXT: The operating system on the runner is
    runs-on: ${{ inputs.os }}

    steps:
      - name: Check out repo ${{ github.repository }}
        uses: actions/checkout@v4

      - name: Set-Up Python version from pyproject.toml
        uses: actions/setup-python@v4
        with:
          # Read python version from a file
          python-version-file: pyproject.toml
          check-latest: true
          architecture: x64

      - name: Display Python version
        id: store
        run: |
          echo $OS_TEXT $RUNNER_OS
          echo "version=$(python --version | tr ' ' '-')" >> $GITHUB_OUTPUT
          python -c "import sys; print(f'🎌 Python-{sys.version}')"
      - run: echo ${{ steps.store.outputs.version }}

      - name: Install dependencies
        run: |
          sudo apt install -y python-is-python3
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Hello! TestRun Report is below...
        run: |
          echo Hello, Starting tests...
          echo ${{ inputs.field }}

      - name: Run Test with pytest
        id: testrun
        env:
          REPORT_XML: TestRun-report-${{ steps.store.outputs.version }}.xml
          REPORT_HTML: TestRun-report-${{ steps.store.outputs.version }}.html
        run: |
          pytest $(echo "--junitxml $REPORT_XML --html $REPORT_HTML --self-contained-html") || true
          echo "path_xml=$(echo $REPORT_XML)" >> $GITHUB_OUTPUT
          echo "path_html=$(echo $REPORT_HTML)" >> $GITHUB_OUTPUT

      - name: Archive TestRun results in HTML
        id: artifact-html
        uses: actions/upload-artifact@v4
        with:
          name: TestRun report $RUNNER_OS ${{ steps.store.outputs.version }}.html
          path: ${{ steps.testrun.outputs.path_html }}
          if-no-files-found: error
          compression-level: 0
          retention-days: 5

      - name: Artifact HTML-ID is ${{ env.ID }}
        env:
          ID: ${{ steps.artifact-html.outputs.artifact-id }}
        run: echo HTML-ID $ID

      - name: Archive TestRun results in XML
        id: artifact-xml
        uses: actions/upload-artifact@v4
        with:
          name: TestRun report $RUNNER_OS ${{ steps.store.outputs.version }}.xml
          path: ${{ steps.testrun.outputs.path_xml }}
          if-no-files-found: error
          compression-level: 0
          retention-days: 5

      - name: Artifact XML-ID is ${{ env.ID }}
        env:
          ID: ${{ steps.artifact-xml.outputs.artifact-id }}
        run: echo XML-ID $ID

      - name: Surface failing tests
        if: always()
        uses: pmeier/pytest-results-action@main
        with:
          title: Test Results for ${{ steps.store.outputs.version }} on $RUNNER_OS
          path: ${{ steps.testrun.outputs.path_xml }}
          fail-on-empty: true
          summary: true
          # Select which results should be included in the report.
          # Follows the same syntax as `pytest -r`
          # Default: fEX
          display-options: fEsxXw
